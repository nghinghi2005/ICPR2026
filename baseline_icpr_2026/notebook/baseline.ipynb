{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Frame CRNN Training Walkthrough\n",
                "\n",
                "This notebook breaks down the training pipeline for a Multi-Frame CRNN model used for text recognition from video tracks. We will go through each component step-by-step."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports and Setup\n",
                "First, we import necessary libraries and set a random seed for reproducibility."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import json\n",
                "import random\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "from torch.amp import autocast, GradScaler\n",
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "import cv2\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "    print(f\"üîí ƒê√£ c·ªë ƒë·ªãnh Seed: {seed}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "Define all hyperparameters and constants in a Config class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    DATA_ROOT = \"/kaggle/input/icpr2026/train\"\n",
                "    IMG_HEIGHT = 32\n",
                "    IMG_WIDTH = 128\n",
                "    CHARS = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ-\"\n",
                "    BATCH_SIZE = 64\n",
                "    LEARNING_RATE = 0.001\n",
                "    EPOCHS = 50\n",
                "    SEED = 42\n",
                "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "    CHAR2IDX = {char: idx + 1 for idx, char in enumerate(CHARS)}\n",
                "    IDX2CHAR = {idx + 1: char for idx, char in enumerate(CHARS)}\n",
                "    NUM_CLASSES = len(CHARS) + 1\n",
                "    VAL_SPLIT_FILE = \"val_tracks.json\"\n",
                "    TEST_SPLIT_FILE = \"test_tracks.json\"\n",
                "    VAL_SIZE = 2000\n",
                "    TEST_SIZE = 2000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Augmentation\n",
                "Define transformations for training (with augmentation) and validation/testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_train_transforms():\n",
                "    return A.Compose([\n",
                "        A.Resize(height=Config.IMG_HEIGHT, width=Config.IMG_WIDTH),\n",
                "\n",
                "        # FIX: cval -> fill\n",
                "        A.Affine(scale=(0.95, 1.05), translate_percent=(0.05, 0.05), rotate=(-5, 5), p=0.5, fill=128),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "\n",
                "        A.RandomBrightnessContrast(p=0.5),\n",
                "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20, p=0.3),\n",
                "\n",
                "        A.CoarseDropout(num_holes_range=(1, 3), hole_height_range=(4, 8), hole_width_range=(4, 8), p=0.3),\n",
                "\n",
                "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_degradation_transforms():\n",
                "    return A.Compose([\n",
                "        A.OneOf([\n",
                "            A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
                "            A.MotionBlur(blur_limit=(3, 7), p=1.0),\n",
                "            A.Defocus(radius=(1, 3), alias_blur=(0.1, 0.3), p=1.0),\n",
                "        ], p=0.8),\n",
                "\n",
                "        A.OneOf([\n",
                "            A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
                "            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1.0),\n",
                "            A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
                "        ], p=0.8),\n",
                "\n",
                "        A.ImageCompression(quality_range=(10, 50), p=0.5),\n",
                "        A.Downscale(scale_range=(0.25, 0.5), p=0.5),\n",
                "    ])\n",
                "\n",
                "def get_val_transforms():\n",
                "    return A.Compose([\n",
                "        A.Resize(height=Config.IMG_HEIGHT, width=Config.IMG_WIDTH),\n",
                "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
                "        ToTensorV2()\n",
                "    ])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset Class\n",
                "Custom Dataset class to handle multi-frame loading and splitting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AdvancedMultiFrameDataset(Dataset):\n",
                "    def __init__(self, root_dir, mode='train'):\n",
                "        self.mode = mode\n",
                "        self.samples = []\n",
                "\n",
                "        if mode == 'train':\n",
                "            self.transform = get_train_transforms()\n",
                "            self.degrade = get_degradation_transforms()\n",
                "        else:\n",
                "            self.transform = get_val_transforms()\n",
                "            self.degrade = None\n",
                "\n",
                "        print(f\"[{mode.upper()}] Scanning: {root_dir}\")\n",
                "        abs_root = os.path.abspath(root_dir)\n",
                "        search_path = os.path.join(abs_root, \"**\", \"track_*\")\n",
                "        all_tracks = sorted(glob.glob(search_path, recursive=True))\n",
                "\n",
                "        if not all_tracks:\n",
                "            print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y data.\")\n",
                "            return\n",
                "\n",
                "        train_tracks = []\n",
                "        val_tracks = []\n",
                "        test_tracks = []\n",
                "\n",
                "        # Check if split files exist\n",
                "        val_exists = os.path.exists(Config.VAL_SPLIT_FILE)\n",
                "        test_exists = os.path.exists(Config.TEST_SPLIT_FILE)\n",
                "\n",
                "        val_ids = set()\n",
                "        test_ids = set()\n",
                "\n",
                "        if val_exists and test_exists:\n",
                "            print(f\"üìÇ Loading splits from '{Config.VAL_SPLIT_FILE}' and '{Config.TEST_SPLIT_FILE}'...\")\n",
                "            try:\n",
                "                with open(Config.VAL_SPLIT_FILE, 'r') as f:\n",
                "                    val_ids = set(json.load(f))\n",
                "                with open(Config.TEST_SPLIT_FILE, 'r') as f:\n",
                "                    test_ids = set(json.load(f))\n",
                "            except:\n",
                "                val_ids = set()\n",
                "                test_ids = set()\n",
                "                print(\"‚ö†Ô∏è L·ªói ƒë·ªçc file split, s·∫Ω t·∫°o l·∫°i.\")\n",
                "\n",
                "            for t in all_tracks:\n",
                "                track_name = os.path.basename(t)\n",
                "                if track_name in val_ids:\n",
                "                    val_tracks.append(t)\n",
                "                elif track_name in test_ids:\n",
                "                    test_tracks.append(t)\n",
                "                else:\n",
                "                    train_tracks.append(t)\n",
                "\n",
                "            # N·∫øu split kh√¥ng kh·ªõp, t·∫°o l·∫°i\n",
                "            if (not val_tracks or not test_tracks) and len(all_tracks) > 0:\n",
                "                print(\"‚ö†Ô∏è File split kh√¥ng kh·ªõp v·ªõi d·ªØ li·ªáu hi·ªán t·∫°i. Chia l·∫°i...\")\n",
                "                val_ids = set()\n",
                "                test_ids = set()\n",
                "\n",
                "        if not val_ids or not test_ids:\n",
                "            print(f\"‚ö†Ô∏è Creating new split: {Config.VAL_SIZE} val, {Config.TEST_SIZE} test...\")\n",
                "            random.Random(Config.SEED).shuffle(all_tracks)\n",
                "\n",
                "            # Chia: val 2000, test 2000, train c√≤n l·∫°i\n",
                "            val_tracks = all_tracks[:Config.VAL_SIZE]\n",
                "            test_tracks = all_tracks[Config.VAL_SIZE:Config.VAL_SIZE + Config.TEST_SIZE]\n",
                "            train_tracks = all_tracks[Config.VAL_SIZE + Config.TEST_SIZE:]\n",
                "\n",
                "            # L∆∞u split files\n",
                "            val_ids = [os.path.basename(t) for t in val_tracks]\n",
                "            test_ids = [os.path.basename(t) for t in test_tracks]\n",
                "            with open(Config.VAL_SPLIT_FILE, 'w') as f:\n",
                "                json.dump(val_ids, f, indent=2)\n",
                "            with open(Config.TEST_SPLIT_FILE, 'w') as f:\n",
                "                json.dump(test_ids, f, indent=2)\n",
                "            print(f\"‚úÖ Saved splits: val={len(val_tracks)}, test={len(test_tracks)}, train={len(train_tracks)}\")\n",
                "\n",
                "        if mode == 'train':\n",
                "            selected_tracks = train_tracks\n",
                "        elif mode == 'val':\n",
                "            selected_tracks = val_tracks\n",
                "        else:  # mode == 'test'\n",
                "            selected_tracks = test_tracks\n",
                "        print(f\"[{mode.upper()}] Loaded {len(selected_tracks)} tracks.\")\n",
                "\n",
                "        for track_path in tqdm(selected_tracks, desc=f\"Indexing {mode}\"):\n",
                "            json_path = os.path.join(track_path, \"annotations.json\")\n",
                "            if not os.path.exists(json_path): continue\n",
                "            try:\n",
                "                with open(json_path, 'r') as f:\n",
                "                    data = json.load(f)\n",
                "                if isinstance(data, list): data = data[0]\n",
                "                label = data.get('plate_text', data.get('license_plate', data.get('text', '')))\n",
                "                if not label: continue\n",
                "\n",
                "                lr_files = sorted(glob.glob(os.path.join(track_path, \"lr-*.png\")) + glob.glob(os.path.join(track_path, \"lr-*.jpg\")))\n",
                "                hr_files = sorted(glob.glob(os.path.join(track_path, \"hr-*.png\")) + glob.glob(os.path.join(track_path, \"hr-*.jpg\")))\n",
                "\n",
                "                if len(lr_files) > 0:\n",
                "                    self.samples.append({\n",
                "                        'lr_paths': lr_files,\n",
                "                        'hr_paths': hr_files,\n",
                "                        'label': label\n",
                "                    })\n",
                "            except: pass\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.samples[idx]\n",
                "        label = item['label']\n",
                "\n",
                "        use_hr = (self.mode == 'train') and (len(item['hr_paths']) > 0) and (random.random() < 0.5)\n",
                "\n",
                "        if use_hr:\n",
                "            img_paths = item['hr_paths']\n",
                "            if len(img_paths) < 5: img_paths = img_paths + [img_paths[-1]] * (5 - len(img_paths))\n",
                "            else: img_paths = img_paths[:5]\n",
                "\n",
                "            images_list = []\n",
                "            for p in img_paths:\n",
                "                image = cv2.imread(p)\n",
                "                if image is None: image = np.zeros((Config.IMG_HEIGHT, Config.IMG_WIDTH, 3), dtype=np.uint8)\n",
                "                else: image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "                if self.degrade:\n",
                "                    image = self.degrade(image=image)['image']\n",
                "                image = self.transform(image=image)['image']\n",
                "                images_list.append(image)\n",
                "        else:\n",
                "            img_paths = item['lr_paths']\n",
                "            if len(img_paths) < 5: img_paths = img_paths + [img_paths[-1]] * (5 - len(img_paths))\n",
                "            else: img_paths = img_paths[:5]\n",
                "\n",
                "            images_list = []\n",
                "            for p in img_paths:\n",
                "                image = cv2.imread(p)\n",
                "                if image is None: image = np.zeros((Config.IMG_HEIGHT, Config.IMG_WIDTH, 3), dtype=np.uint8)\n",
                "                else: image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "                image = self.transform(image=image)['image']\n",
                "                images_list.append(image)\n",
                "\n",
                "        images_tensor = torch.stack(images_list, dim=0)\n",
                "        target = [Config.CHAR2IDX[c] for c in label if c in Config.CHAR2IDX]\n",
                "        if len(target) == 0: target = [0]\n",
                "\n",
                "        return images_tensor, torch.tensor(target, dtype=torch.long), len(target), label\n",
                "\n",
                "    @staticmethod\n",
                "    def collate_fn(batch):\n",
                "        images, targets, target_lengths, labels_text = zip(*batch)\n",
                "        images = torch.stack(images, 0)\n",
                "        targets = torch.cat(targets)\n",
                "        target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
                "        return images, targets, target_lengths, labels_text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Architecture\n",
                "Define the Attention Fusion module and the main CRNN model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AttentionFusion(nn.Module):\n",
                "    def __init__(self, channels):\n",
                "        super().__init__()\n",
                "        self.score_net = nn.Sequential(\n",
                "            nn.Conv2d(channels, channels // 8, kernel_size=1),\n",
                "            nn.ReLU(True),\n",
                "            nn.Conv2d(channels // 8, 1, kernel_size=1)\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        b_frames, c, h, w = x.size()\n",
                "        b_size = b_frames // 5\n",
                "        x_view = x.view(b_size, 5, c, w)\n",
                "        scores = self.score_net(x).view(b_size, 5, 1, w)\n",
                "        return torch.sum(x_view * F.softmax(scores, dim=1), dim=1)\n",
                "\n",
                "class MultiFrameCRNN(nn.Module):\n",
                "    def __init__(self, num_classes, hidden_size=256):\n",
                "        super().__init__()\n",
                "        self.cnn = nn.Sequential(\n",
                "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
                "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
                "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
                "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 2), (2, 1), (0, 1)),\n",
                "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
                "            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 2), (2, 1), (0, 1)),\n",
                "            nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)\n",
                "        )\n",
                "        self.fusion = AttentionFusion(channels=512)\n",
                "        self.rnn = nn.Sequential(nn.LSTM(512, hidden_size, bidirectional=True, batch_first=True, num_layers=2, dropout=0.25))\n",
                "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        b, t, c, h, w = x.size()\n",
                "        x = x.view(b * t, c, h, w)\n",
                "        feat = self.cnn(x)\n",
                "        fused = self.fusion(feat)\n",
                "        out = self.fc(self.rnn(fused.permute(0, 2, 1))[0])\n",
                "        return out.log_softmax(2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Utility Functions\n",
                "Helper function to decode model predictions into text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def decode_predictions(preds, idx2char):\n",
                "    result_list = []\n",
                "    for p in preds:\n",
                "        pred_str = \"\"\n",
                "        last_char = 0\n",
                "        for char_idx in p:\n",
                "            c = char_idx.item()\n",
                "            if c != 0 and c != last_char: pred_str += idx2char[c]\n",
                "            last_char = c\n",
                "        result_list.append(pred_str)\n",
                "    return result_list"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Pipeline\n",
                "The main training loop, including validation and testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_pipeline():\n",
                "    seed_everything(Config.SEED)\n",
                "    print(f\"üöÄ TRAINING START | Device: {Config.DEVICE}\")\n",
                "\n",
                "    if not os.path.exists(Config.DATA_ROOT):\n",
                "        print(\"‚ùå L·ªñI: Sai ƒë∆∞·ªùng d·∫´n DATA_ROOT\")\n",
                "        return\n",
                "\n",
                "    train_ds = AdvancedMultiFrameDataset(Config.DATA_ROOT, mode='train')\n",
                "    val_ds = AdvancedMultiFrameDataset(Config.DATA_ROOT, mode='val')\n",
                "    test_ds = AdvancedMultiFrameDataset(Config.DATA_ROOT, mode='test')\n",
                "\n",
                "    if len(train_ds) == 0:\n",
                "        print(\"‚ùå Dataset Train r·ªóng!\")\n",
                "        return\n",
                "\n",
                "    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
                "                               collate_fn=AdvancedMultiFrameDataset.collate_fn, num_workers=10, pin_memory=True)\n",
                "\n",
                "    if len(val_ds) > 0:\n",
                "        val_loader = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
                "                                collate_fn=AdvancedMultiFrameDataset.collate_fn, num_workers=10, pin_memory=True)\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Validation Set r·ªóng. S·∫Ω b·ªè qua b∆∞·ªõc validate.\")\n",
                "        val_loader = None\n",
                "\n",
                "    if len(test_ds) > 0:\n",
                "        test_loader = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
                "                                 collate_fn=AdvancedMultiFrameDataset.collate_fn, num_workers=10, pin_memory=True)\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Test Set r·ªóng.\")\n",
                "        test_loader = None\n",
                "\n",
                "    model = MultiFrameCRNN(num_classes=Config.NUM_CLASSES).to(Config.DEVICE)\n",
                "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=Config.LEARNING_RATE,\n",
                "                                              steps_per_epoch=len(train_loader), epochs=Config.EPOCHS)\n",
                "    scaler = GradScaler()\n",
                "\n",
                "    best_acc = 0.0\n",
                "    for epoch in range(Config.EPOCHS):\n",
                "        model.train()\n",
                "        epoch_loss = 0\n",
                "\n",
                "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}/{Config.EPOCHS}\")\n",
                "        for images, targets, target_lengths, _ in pbar:\n",
                "            images = images.to(Config.DEVICE)\n",
                "            targets = targets.to(Config.DEVICE)\n",
                "\n",
                "            optimizer.zero_grad(set_to_none=True)\n",
                "\n",
                "            with autocast('cuda'):\n",
                "                preds = model(images)\n",
                "                preds_permuted = preds.permute(1, 0, 2)\n",
                "                input_lengths = torch.full(size=(images.size(0),), fill_value=preds.size(1), dtype=torch.long)\n",
                "                loss = criterion(preds_permuted, targets, input_lengths, target_lengths)\n",
                "\n",
                "            scaler_scale_before = scaler.get_scale()\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "\n",
                "            # Ch·ªâ step scheduler n·∫øu optimizer ƒë√£ step (scale kh√¥ng b·ªã gi·∫£m/reset)\n",
                "            # ƒêi·ªÅu n√†y fix warning \"Detected call of lr_scheduler.step() before optimizer.step()\"\n",
                "            if scaler.get_scale() >= scaler_scale_before:\n",
                "                scheduler.step()\n",
                "\n",
                "            epoch_loss += loss.item()\n",
                "            pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
                "\n",
                "        avg_train_loss = epoch_loss / len(train_loader)\n",
                "\n",
                "        val_acc = 0\n",
                "        avg_val_loss = 0\n",
                "\n",
                "        if val_loader:\n",
                "            model.eval()\n",
                "            val_loss = 0\n",
                "            total_correct = 0\n",
                "            total_samples = 0\n",
                "\n",
                "            with torch.no_grad():\n",
                "                for images, targets, target_lengths, labels_text in val_loader:\n",
                "                    images = images.to(Config.DEVICE)\n",
                "                    targets = targets.to(Config.DEVICE)\n",
                "                    preds = model(images)\n",
                "\n",
                "                    loss = criterion(preds.permute(1, 0, 2), targets,\n",
                "                                     torch.full((images.size(0),), preds.size(1), dtype=torch.long), target_lengths)\n",
                "                    val_loss += loss.item()\n",
                "\n",
                "                    decoded = decode_predictions(torch.argmax(preds, dim=2), Config.IDX2CHAR)\n",
                "                    for i in range(len(labels_text)):\n",
                "                        if decoded[i] == labels_text[i]:\n",
                "                            total_correct += 1\n",
                "                    total_samples += len(labels_text)\n",
                "\n",
                "            avg_val_loss = val_loss / len(val_loader)\n",
                "            val_acc = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
                "\n",
                "        print(f\"Result: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
                "\n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            torch.save(model.state_dict(), \"best_model.pth\")\n",
                "            print(f\" -> ‚≠ê Saved Best Model! ({val_acc:.2f}%)\")\n",
                "\n",
                "    # ==========================================\n",
                "    # INFERENCE ON TEST SET\n",
                "    # ==========================================\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üß™ EVALUATING ON TEST SET...\")\n",
                "    print(\"=\"*60)\n",
                "\n",
                "    if test_loader and os.path.exists(\"best_model.pth\"):\n",
                "        # Load best model\n",
                "        model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
                "        model.eval()\n",
                "\n",
                "        test_correct = 0\n",
                "        test_total = 0\n",
                "        test_char_correct = 0\n",
                "        test_char_total = 0\n",
                "\n",
                "        results = []  # L∆∞u k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for images, targets, target_lengths, labels_text in tqdm(test_loader, desc=\"Testing\"):\n",
                "                images = images.to(Config.DEVICE)\n",
                "                preds = model(images)\n",
                "                decoded = decode_predictions(torch.argmax(preds, dim=2), Config.IDX2CHAR)\n",
                "\n",
                "                for i in range(len(labels_text)):\n",
                "                    gt = labels_text[i]\n",
                "                    pred = decoded[i]\n",
                "\n",
                "                    # Exact match accuracy\n",
                "                    if pred == gt:\n",
                "                        test_correct += 1\n",
                "                    test_total += 1\n",
                "\n",
                "                    # Character-level accuracy\n",
                "                    for j in range(max(len(gt), len(pred))):\n",
                "                        test_char_total += 1\n",
                "                        if j < len(gt) and j < len(pred) and gt[j] == pred[j]:\n",
                "                            test_char_correct += 1\n",
                "\n",
                "                    # L∆∞u m·ªôt s·ªë k·∫øt qu·∫£ sai ƒë·ªÉ debug\n",
                "                    if pred != gt and len(results) < 20:\n",
                "                        results.append({'gt': gt, 'pred': pred})\n",
                "\n",
                "        test_acc = (test_correct / test_total) * 100 if test_total > 0 else 0\n",
                "        char_acc = (test_char_correct / test_char_total) * 100 if test_char_total > 0 else 0\n",
                "\n",
                "        print(f\"\\nüìä TEST RESULTS:\")\n",
                "        print(f\"   ‚Ä¢ Exact Match Accuracy: {test_acc:.2f}% ({test_correct}/{test_total})\")\n",
                "        print(f\"   ‚Ä¢ Character Accuracy:   {char_acc:.2f}%\")\n",
                "        print(f\"   ‚Ä¢ Best Val Accuracy:    {best_acc:.2f}%\")\n",
                "\n",
                "        if results:\n",
                "            print(f\"\\nüîç Sample Errors (first 10):\")\n",
                "            for i, r in enumerate(results[:10]):\n",
                "                print(f\"   {i+1}. GT: '{r['gt']}' | Pred: '{r['pred']}'\")\n",
                "\n",
                "        # L∆∞u k·∫øt qu·∫£ v√†o file\n",
                "        test_results = {\n",
                "            'test_accuracy': test_acc,\n",
                "            'char_accuracy': char_acc,\n",
                "            'best_val_accuracy': best_acc,\n",
                "            'total_samples': test_total,\n",
                "            'correct_samples': test_correct,\n",
                "            'sample_errors': results\n",
                "        }\n",
                "        with open('test_results.json', 'w') as f:\n",
                "            json.dump(test_results, f, indent=2)\n",
                "        print(f\"\\nüíæ Results saved to 'test_results.json'\")\n",
                "\n",
                "    else:\n",
                "        if not test_loader:\n",
                "            print(\"‚ùå Test loader kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
                "        if not os.path.exists(\"best_model.pth\"):\n",
                "            print(\"‚ùå Kh√¥ng t√¨m th·∫•y best_model.pth!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Run Training\n",
                "Execute the training pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîí ƒê√£ c·ªë ƒë·ªãnh Seed: 42\n",
                        "üöÄ TRAINING START | Device: cuda\n",
                        "[TRAIN] Scanning: /kaggle/input/icpr2026/train\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_20/3928404161.py:78: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
                        "  A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è Creating new split: 2000 val, 2000 test...\n",
                        "‚úÖ Saved splits: val=2000, test=2000, train=16000\n",
                        "[TRAIN] Loaded 16000 tracks.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Indexing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16000/16000 [01:42<00:00, 155.76it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[VAL] Scanning: /kaggle/input/icpr2026/train\n",
                        "üìÇ Loading splits from 'val_tracks.json' and 'test_tracks.json'...\n",
                        "[VAL] Loaded 2000 tracks.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Indexing val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:11<00:00, 168.32it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[TEST] Scanning: /kaggle/input/icpr2026/train\n",
                        "üìÇ Loading splits from 'val_tracks.json' and 'test_tracks.json'...\n",
                        "[TEST] Loaded 2000 tracks.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Indexing test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:12<00:00, 155.46it/s]\n",
                        "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
                        "  warnings.warn(\n",
                        "Ep 1/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:18<00:00,  3.19it/s, loss=3.33, lr=5.03e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 4.6946 | Val Loss: 3.3131 | Val Acc: 0.00%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 2/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:08<00:00,  3.63it/s, loss=3.06, lr=8.12e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 3.1803 | Val Loss: 3.0592 | Val Acc: 0.00%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 3/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:04<00:00,  3.86it/s, loss=2.97, lr=0.000131]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 2.9768 | Val Loss: 2.9191 | Val Acc: 0.00%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 4/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:04<00:00,  3.90it/s, loss=2.79, lr=0.000198]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 2.8559 | Val Loss: 2.8216 | Val Acc: 0.00%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 5/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.05it/s, loss=2.09, lr=0.000279]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 2.5265 | Val Loss: 2.2764 | Val Acc: 0.00%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 6/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.10it/s, loss=1.27, lr=0.000371]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 1.6586 | Val Loss: 1.4478 | Val Acc: 2.15%\n",
                        " -> ‚≠ê Saved Best Model! (2.15%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 7/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.08it/s, loss=0.863, lr=0.000469]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.9668 | Val Loss: 0.9246 | Val Acc: 20.95%\n",
                        " -> ‚≠ê Saved Best Model! (20.95%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 8/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.11it/s, loss=0.449, lr=0.00057]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.6265 | Val Loss: 0.7480 | Val Acc: 34.10%\n",
                        " -> ‚≠ê Saved Best Model! (34.10%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 9/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.12it/s, loss=0.453, lr=0.000668]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.4856 | Val Loss: 0.6336 | Val Acc: 43.40%\n",
                        " -> ‚≠ê Saved Best Model! (43.40%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 10/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.10it/s, loss=0.34, lr=0.00076]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.4097 | Val Loss: 0.6531 | Val Acc: 39.20%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 11/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.12it/s, loss=0.317, lr=0.000841]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.3700 | Val Loss: 0.5297 | Val Acc: 50.15%\n",
                        " -> ‚≠ê Saved Best Model! (50.15%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 12/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.13it/s, loss=0.519, lr=0.000908]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.3347 | Val Loss: 0.5484 | Val Acc: 48.35%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 13/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.12it/s, loss=0.38, lr=0.000958]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2919 | Val Loss: 0.5096 | Val Acc: 49.85%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 14/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.14it/s, loss=0.194, lr=0.000989]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2701 | Val Loss: 0.4506 | Val Acc: 55.15%\n",
                        " -> ‚≠ê Saved Best Model! (55.15%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 15/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.17it/s, loss=0.24, lr=0.001]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2564 | Val Loss: 0.4712 | Val Acc: 53.85%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 16/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.17it/s, loss=0.201, lr=0.000998]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2354 | Val Loss: 0.4694 | Val Acc: 51.75%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 17/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.13it/s, loss=0.216, lr=0.000992]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2313 | Val Loss: 0.4173 | Val Acc: 57.10%\n",
                        " -> ‚≠ê Saved Best Model! (57.10%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 18/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.13it/s, loss=0.201, lr=0.000982]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2171 | Val Loss: 0.3789 | Val Acc: 63.45%\n",
                        " -> ‚≠ê Saved Best Model! (63.45%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 19/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.16it/s, loss=0.101, lr=0.000968]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2068 | Val Loss: 0.3943 | Val Acc: 60.20%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 20/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.21it/s, loss=0.148, lr=0.000951]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.2014 | Val Loss: 0.4108 | Val Acc: 59.65%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 21/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.13it/s, loss=0.108, lr=0.000929]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1881 | Val Loss: 0.3742 | Val Acc: 61.55%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 22/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.13it/s, loss=0.162, lr=0.000905]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1795 | Val Loss: 0.3755 | Val Acc: 63.70%\n",
                        " -> ‚≠ê Saved Best Model! (63.70%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 23/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.22it/s, loss=0.0898, lr=0.000877]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1749 | Val Loss: 0.4149 | Val Acc: 61.70%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 24/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.17it/s, loss=0.127, lr=0.000846]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1676 | Val Loss: 0.3687 | Val Acc: 62.45%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 25/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.11it/s, loss=0.084, lr=0.000812]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1605 | Val Loss: 0.3655 | Val Acc: 63.60%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 26/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.11it/s, loss=0.118, lr=0.000776]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1537 | Val Loss: 0.3430 | Val Acc: 67.35%\n",
                        " -> ‚≠ê Saved Best Model! (67.35%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 27/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.12it/s, loss=0.12, lr=0.000737]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1467 | Val Loss: 0.3576 | Val Acc: 65.80%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 28/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.18it/s, loss=0.134, lr=0.000697]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1353 | Val Loss: 0.3492 | Val Acc: 67.00%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 29/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.34it/s, loss=0.138, lr=0.000655]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1322 | Val Loss: 0.3459 | Val Acc: 66.45%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 30/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.34it/s, loss=0.0995, lr=0.000611]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1204 | Val Loss: 0.3335 | Val Acc: 68.05%\n",
                        " -> ‚≠ê Saved Best Model! (68.05%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 31/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.25it/s, loss=0.181, lr=0.000567]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1212 | Val Loss: 0.3556 | Val Acc: 66.10%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 32/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.22it/s, loss=0.105, lr=0.000523]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1104 | Val Loss: 0.3333 | Val Acc: 67.35%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 33/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.24it/s, loss=0.0689, lr=0.000478]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.1040 | Val Loss: 0.3223 | Val Acc: 68.70%\n",
                        " -> ‚≠ê Saved Best Model! (68.70%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 34/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.32it/s, loss=0.107, lr=0.000433]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0919 | Val Loss: 0.3104 | Val Acc: 72.50%\n",
                        " -> ‚≠ê Saved Best Model! (72.50%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 35/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.30it/s, loss=0.047, lr=0.000389]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0896 | Val Loss: 0.3124 | Val Acc: 70.90%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 36/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.25it/s, loss=0.121, lr=0.000346]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0846 | Val Loss: 0.3219 | Val Acc: 70.20%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 37/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.17it/s, loss=0.0714, lr=0.000304]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0742 | Val Loss: 0.3171 | Val Acc: 72.05%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 38/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.16it/s, loss=0.0421, lr=0.000263]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0727 | Val Loss: 0.3119 | Val Acc: 71.05%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 39/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.26it/s, loss=0.0601, lr=0.000225]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0650 | Val Loss: 0.3110 | Val Acc: 72.45%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 40/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.36it/s, loss=0.0799, lr=0.000189]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0621 | Val Loss: 0.3122 | Val Acc: 72.25%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 41/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.34it/s, loss=0.0373, lr=0.000155]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0561 | Val Loss: 0.3142 | Val Acc: 72.50%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 42/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.15it/s, loss=0.0338, lr=0.000124]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0530 | Val Loss: 0.3141 | Val Acc: 71.65%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 43/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.22it/s, loss=0.0898, lr=9.58e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0474 | Val Loss: 0.3064 | Val Acc: 73.40%\n",
                        " -> ‚≠ê Saved Best Model! (73.40%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 44/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.30it/s, loss=0.0777, lr=7.11e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0452 | Val Loss: 0.3103 | Val Acc: 73.45%\n",
                        " -> ‚≠ê Saved Best Model! (73.45%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 45/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.26it/s, loss=0.0517, lr=4.98e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0435 | Val Loss: 0.3142 | Val Acc: 72.75%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 46/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.29it/s, loss=0.0429, lr=3.21e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0427 | Val Loss: 0.3125 | Val Acc: 73.35%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 47/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.39it/s, loss=0.045, lr=1.82e-5]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0412 | Val Loss: 0.3113 | Val Acc: 73.40%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 48/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.28it/s, loss=0.0303, lr=8.14e-6]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0419 | Val Loss: 0.3090 | Val Acc: 73.50%\n",
                        " -> ‚≠ê Saved Best Model! (73.50%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 49/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.33it/s, loss=0.0365, lr=2.07e-6]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0410 | Val Loss: 0.3099 | Val Acc: 73.45%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 50/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.35it/s, loss=0.0205, lr=4.29e-9]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result: Train Loss: 0.0403 | Val Loss: 0.3097 | Val Acc: 73.35%\n",
                        "\n",
                        "============================================================\n",
                        "üß™ EVALUATING ON TEST SET...\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.14it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìä TEST RESULTS:\n",
                        "   ‚Ä¢ Exact Match Accuracy: 73.20% (1464/2000)\n",
                        "   ‚Ä¢ Character Accuracy:   92.59%\n",
                        "   ‚Ä¢ Best Val Accuracy:    73.50%\n",
                        "\n",
                        "üîç Sample Errors (first 10):\n",
                        "   1. GT: 'POC2067' | Pred: 'POC2087'\n",
                        "   2. GT: 'AVX8286' | Pred: 'AVR8088'\n",
                        "   3. GT: 'AZV2616' | Pred: 'AZY2616'\n",
                        "   4. GT: 'AXY9931' | Pred: 'AVV9881'\n",
                        "   5. GT: 'AZO1492' | Pred: 'AZQ1492'\n",
                        "   6. GT: 'ATB9207' | Pred: 'ATB3559'\n",
                        "   7. GT: 'ARE5453' | Pred: 'ARE5457'\n",
                        "   8. GT: 'AXQ2352' | Pred: 'AYS2952'\n",
                        "   9. GT: 'AVJ7030' | Pred: 'AVJ7830'\n",
                        "   10. GT: 'BBL0621' | Pred: 'BBE0621'\n",
                        "\n",
                        "üíæ Results saved to 'test_results.json'\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    train_pipeline()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
