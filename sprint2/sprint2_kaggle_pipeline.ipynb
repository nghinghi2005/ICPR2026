{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6016a7",
   "metadata": {},
   "source": [
    "# Sprint 2 – End-to-End Kaggle Notebook (SR → OCR-CTC)\n",
    "\n",
    "This notebook is **standalone** (no dependency on other notebooks). It runs the full pipeline:\n",
    "\n",
    "1. **Super-Resolution (HR+)**: train a small SR model on synthetic pairs and export HR+ frames from LR frames.\n",
    "2. **OCR (Multi-frame CRNN + CTC)**: train the recognizer.\n",
    "3. **Brazil/Mercosur CTC decoding**: beam search + template constraints + confusion-map post-processing.\n",
    "\n",
    "## Why the Brazil/Mercosur constraint layer?\n",
    "In our meeting/discussion notes, the main pain point is CTC decoding errors on ambiguous glyphs (e.g., **O/0**, **I/1**, **B/8**, **Z/2**) and merged characters. Instead of only relying on the network, we add a lightweight, **rule-based** decoding layer that enforces valid plate templates:\n",
    "\n",
    "- **Brazil old**: `LLLDDDD` (optionally `LLL-DDDD`)\n",
    "- **Mercosur (Brazil)**: `LLLDLDD` (optionally `LLL-DLDD`)\n",
    "\n",
    "## Outputs (safe space)\n",
    "All checkpoints and exports are written to **`/kaggle/working/sprint2_outputs/`** (persist via Kaggle *Save & Commit*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup imports and paths ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def add_to_syspath_front(p: Path) -> None:\n",
    "    p = p.resolve()\n",
    "    ps = str(p)\n",
    "    if ps in sys.path:\n",
    "        sys.path.remove(ps)\n",
    "    sys.path.insert(0, ps)\n",
    "\n",
    "# Kaggle: repo dataset name is expected to be 'icpr2026-repo'\n",
    "repo_root = Path('/kaggle/input/icpr2026-repo')\n",
    "if repo_root.exists():\n",
    "    add_to_syspath_front(repo_root)\n",
    "else:\n",
    "    # fallback: find any dataset that contains a sprint2 folder\n",
    "    base = Path('/kaggle/input')\n",
    "    if base.exists():\n",
    "        for d in base.iterdir():\n",
    "            if (d / 'sprint2').exists():\n",
    "                add_to_syspath_front(d)\n",
    "                repo_root = d\n",
    "                break\n",
    "\n",
    "# Local/dev: if running from a folder that contains ./sprint2\n",
    "cwd = Path.cwd()\n",
    "if (cwd / 'sprint2').exists():\n",
    "    add_to_syspath_front(cwd)\n",
    "\n",
    "print('CWD:', cwd)\n",
    "print('repo_root:', repo_root)\n",
    "print('repo_root exists:', repo_root.exists())\n",
    "print('repo_root in sys.path:', str(repo_root.resolve()) in [str(Path(x).resolve()) for x in sys.path if isinstance(x, str)])\n",
    "print('sys.path[0:5]:', sys.path[:5])\n",
    "\n",
    "# Verify the import resolves from the expected location\n",
    "import sprint2\n",
    "print('sprint2 imported from:', sprint2.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c91296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Install SR deps if needed (Kaggle + Internet ON only) ---\n",
    "# This is ONLY required when you set SRConfig.use_pretrained=True and want Real-ESRGAN weights via BasicSR.\n",
    "# If Kaggle internet is OFF, do NOT run this (it will fail). Instead, upload weights + dependencies or disable pretrained mode.\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import subprocess\n",
    "import socket\n",
    "\n",
    "def _internet_enabled(timeout_sec: float = 2.0) -> bool:\n",
    "    # Kaggle doesn't expose a perfect flag; use a quick TCP probe.\n",
    "    # If this returns False, pip install from PyPI will almost certainly fail.\n",
    "    try:\n",
    "        sock = socket.create_connection(('pypi.org', 443), timeout=timeout_sec)\n",
    "        sock.close()\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def _module_exists(name: str) -> bool:\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "need_basicsr = not _module_exists('basicsr')\n",
    "need_realesrgan = not _module_exists('realesrgan')\n",
    "\n",
    "print('basicsr installed:', not need_basicsr)\n",
    "print('realesrgan installed:', not need_realesrgan)\n",
    "\n",
    "internet_ok = _internet_enabled()\n",
    "print('internet probe (pypi.org:443):', internet_ok)\n",
    "\n",
    "if need_basicsr or need_realesrgan:\n",
    "    if not internet_ok:\n",
    "        print('Internet appears OFF/unavailable. Skipping pip install.')\n",
    "        print('Options:')\n",
    "        print('  1) Enable Internet in Kaggle notebook settings and re-run this cell')\n",
    "        print('  2) Upload a Kaggle Dataset that contains the weights file and set sr_cfg.pretrained_path')\n",
    "        print(\"  3) Set sr_cfg.use_pretrained = False to use the built-in SR model\")\n",
    "    else:\n",
    "        pkgs = []\n",
    "        if need_basicsr:\n",
    "            pkgs.append('basicsr')\n",
    "        if need_realesrgan:\n",
    "            pkgs.append('realesrgan')\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', '-q'] + pkgs\n",
    "        print('Installing:', pkgs)\n",
    "        try:\n",
    "            subprocess.check_call(cmd)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print('pip install failed:', repr(e))\n",
    "            print('Tip: in Kaggle you may need to enable Internet, or pin versions compatible with the default torch.')\n",
    "            raise\n",
    "        print('Install done. Verifying imports...')\n",
    "        print('basicsr import OK:', _module_exists('basicsr'))\n",
    "        print('realesrgan import OK:', _module_exists('realesrgan'))\n",
    "        print('Now re-run the SR cell.')\n",
    "else:\n",
    "    print('All optional SR deps already installed; nothing to do.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ca9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our local sprint2 modules\n",
    "from sprint2.paths import get_paths\n",
    "from sprint2.config import OCRConfig, SRConfig\n",
    "from sprint2.sr_train_export import train_sr, export_hr_plus\n",
    "from sprint2.ocr_train import train_ocr\n",
    "\n",
    "paths = get_paths('sprint2_outputs')\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eccfd2",
   "metadata": {},
   "source": [
    "## 1) Configure dataset paths (Kaggle)\n",
    "Set `DATA_ROOT` to your competition dataset train folder containing `Scenario-*/*/track_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data root: supports uploaded zip OR Kaggle dataset folder ---\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# If your dataset isn't showing up under /kaggle/input, it is NOT attached to this notebook session.\n",
    "# Kaggle mounts only the datasets you add via the right panel: \"Add Data\".\n",
    "# After adding a dataset, you often need \"Restart Session\" for it to appear under /kaggle/input.\n",
    "\n",
    "PREFERRED_DATA_ROOT = Path('/kaggle/input/icpr-2026-train/train')\n",
    "PREFERRED_DATASET_ROOT = Path('/kaggle/input/icpr-2026-train')\n",
    "\n",
    "# Manual override (recommended when debugging):\n",
    "# - Set in a notebook cell before this runs: os.environ['ICPR_DATA_ROOT'] = '/kaggle/input/<dataset>/train'\n",
    "# - Or set in Kaggle Notebook \"Add-ons\" -> \"Secrets\" / environment variables (if available)\n",
    "ENV_DATA_ROOT = os.environ.get('ICPR_DATA_ROOT', '').strip()\n",
    "DATASET_SLUG = os.environ.get('ICPR_DATASET_SLUG', '').strip()  # e.g. 'icpr-2026-train'\n",
    "\n",
    "# Option A: add your dataset under /kaggle/input/ (Kaggle Dataset or Competition data).\n",
    "# Option B: upload a single .zip file to the notebook and unzip it into /kaggle/working.\n",
    "ZIP_PATH = os.environ.get('ICPR_ZIP_PATH', '').strip()  # e.g. '/kaggle/input/my-upload/train.zip' or '/kaggle/working/train.zip'\n",
    "UNZIP_DIR = Path(os.environ.get('ICPR_UNZIP_DIR', '/kaggle/working/icpr_data'))\n",
    "\n",
    "def _looks_like_train_dir(p: Path) -> bool:\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        return False\n",
    "    scenarios = list(p.glob('Scenario-*'))\n",
    "    if not scenarios:\n",
    "        return False\n",
    "    # fast structural check: at least one track_* somewhere under a scenario\n",
    "    for s in scenarios[:3]:\n",
    "        try:\n",
    "            if any(s.rglob('track_*')):\n",
    "                return True\n",
    "        except Exception:\n",
    "            # rglob can sometimes fail on very large trees; still accept scenario presence\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _candidate_train_roots() -> list[Path]:\n",
    "    cands: list[Path] = []\n",
    "    if ENV_DATA_ROOT:\n",
    "        cands.append(Path(ENV_DATA_ROOT))\n",
    "    if DATASET_SLUG:\n",
    "        cands.append(Path('/kaggle/input') / DATASET_SLUG / 'train')\n",
    "        cands.append(Path('/kaggle/input') / DATASET_SLUG / 'train' / 'train')\n",
    "        cands.append(Path('/kaggle/input') / DATASET_SLUG)\n",
    "\n",
    "    # Common layouts for the expected dataset name\n",
    "    cands.extend([\n",
    "        PREFERRED_DATA_ROOT,\n",
    "        PREFERRED_DATASET_ROOT / 'train',\n",
    "        PREFERRED_DATASET_ROOT / 'train' / 'train',\n",
    "        PREFERRED_DATA_ROOT / 'train',\n",
    "        Path('/kaggle/input/icpr-2026-train') / 'train',\n",
    "        Path('/kaggle/input/icpr-2026-train') / 'train' / 'train',\n",
    "    ])\n",
    "\n",
    "    # Also: any /kaggle/input/*/{train,Train}\n",
    "    base = Path('/kaggle/input')\n",
    "    if base.exists():\n",
    "        for d in base.iterdir():\n",
    "            if d.is_dir():\n",
    "                cands.append(d / 'train')\n",
    "                cands.append(d / 'Train')\n",
    "                cands.append(d / 'train' / 'train')\n",
    "\n",
    "    # De-dup while preserving order\n",
    "    seen = set()\n",
    "    out: list[Path] = []\n",
    "    for c in cands:\n",
    "        s = str(c)\n",
    "        if s not in seen:\n",
    "            out.append(c)\n",
    "            seen.add(s)\n",
    "    return out\n",
    "\n",
    "def _auto_find_train_root() -> Path:\n",
    "    for c in _candidate_train_roots():\n",
    "        if _looks_like_train_dir(c):\n",
    "            return c\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not auto-detect train root under /kaggle/input. \"\n",
    "        \"Attach the dataset (Add Data) and restart session, or set ICPR_DATA_ROOT.\"\n",
    "    )\n",
    "\n",
    "def _maybe_unzip(zip_path: str, unzip_dir: Path) -> None:\n",
    "    if not zip_path:\n",
    "        return\n",
    "    z = Path(zip_path)\n",
    "    if not z.exists():\n",
    "        raise FileNotFoundError(f'ZIP_PATH not found: {z}')\n",
    "    unzip_dir.mkdir(parents=True, exist_ok=True)\n",
    "    marker = unzip_dir / '.unzipped.ok'\n",
    "    if marker.exists():\n",
    "        return\n",
    "    print(f'Unzipping {z} -> {unzip_dir} ...')\n",
    "    with zipfile.ZipFile(z, 'r') as zf:\n",
    "        zf.extractall(unzip_dir)\n",
    "    marker.write_text('ok', encoding='utf-8')\n",
    "    print('Unzip done.')\n",
    "\n",
    "# If user provided a zip, unzip once into /kaggle/working and use it\n",
    "if ZIP_PATH:\n",
    "    _maybe_unzip(ZIP_PATH, UNZIP_DIR)\n",
    "    for cand in [UNZIP_DIR / 'train', UNZIP_DIR]:\n",
    "        if _looks_like_train_dir(cand):\n",
    "            DATA_ROOT = str(cand)\n",
    "            break\n",
    "    else:\n",
    "        found = None\n",
    "        for t in UNZIP_DIR.rglob('train'):\n",
    "            if _looks_like_train_dir(t):\n",
    "                found = t\n",
    "                break\n",
    "        if found is None:\n",
    "            raise FileNotFoundError(f'Unzipped but could not find a valid train folder under {UNZIP_DIR}')\n",
    "        DATA_ROOT = str(found)\n",
    "else:\n",
    "    try:\n",
    "        DATA_ROOT = str(_auto_find_train_root())\n",
    "    except FileNotFoundError:\n",
    "        # Print diagnostics to help you correct the path quickly\n",
    "        print('--- Diagnostics ---')\n",
    "        print('ENV ICPR_DATA_ROOT =', ENV_DATA_ROOT or '(not set)')\n",
    "        print('ENV ICPR_DATASET_SLUG =', DATASET_SLUG or '(not set)')\n",
    "        print('PREFERRED_DATASET_ROOT exists:', PREFERRED_DATASET_ROOT.exists())\n",
    "        base = Path('/kaggle/input')\n",
    "        if base.exists():\n",
    "            mounted = [p.name for p in base.iterdir() if p.is_dir()]\n",
    "            print('Datasets mounted in /kaggle/input:', mounted)\n",
    "            if 'icpr-2026-train' not in mounted:\n",
    "                print('NOTE: icpr-2026-train is NOT mounted. Use Kaggle right panel -> Add Data -> select your train dataset, then Restart Session.')\n",
    "        raise\n",
    "\n",
    "print('DATA_ROOT =', DATA_ROOT)\n",
    "assert Path(DATA_ROOT).exists(), f'Not found: {DATA_ROOT}'\n",
    "print('Scenario-A exists:', (Path(DATA_ROOT) / 'Scenario-A').exists())\n",
    "\n",
    "# Outputs are written here (safe writable space)\n",
    "OUT_ROOT = paths.out_root\n",
    "OUT_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5447de",
   "metadata": {},
   "source": [
    "## 2) Super-Resolution (SR) → Export HR+ (distribution-consistent)\n",
    "We train a stronger SR model (**RRDBNetLite / ESRGAN-lite**) and add a light **degradation-consistency** loss so that when we *downgrade the SR output back to LR*, it matches the **real LR distribution** better.\n",
    "\n",
    "### Resolution logic (keep it consistent)\n",
    "- **LR → HR** scale is inferred from real paired `lr-*` and `hr-*` frame sizes (usually `x2`).\n",
    "- We export **HR+** at `x<scale>` relative to the chosen source frames.\n",
    "\n",
    "### Which source should you export from?\n",
    "- `SR_SOURCE='lr'` (**recommended for distribution matching**): export HR+ from `lr-*` so `downsample(HR+) ≈ original LR`.\n",
    "- `SR_SOURCE='hr'` (optional): export HR+ from `hr-*` for extra detail beyond HR; use when you explicitly want HR→HR+.\n",
    "\n",
    "Exports go to: `/kaggle/working/sprint2_outputs/hr_plus/x<scale>/...`\n",
    "You can lower runtime by setting `LIMIT_TRACKS` (export only N track folders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SR config tuned to be notebook-friendly\n",
    "sr_cfg = SRConfig(\n",
    "    scale=2,\n",
    "    # If you enable pretrained, the code will try to build a checkpoint from external weights (Real-ESRGAN).\n",
    "    # If deps/weights/internet are not available, it will fall back to training internal rrdb_lite.\n",
    "    use_pretrained=True,\n",
    "    pretrained_name='realesrgan_x2plus',\n",
    "    pretrained_path='',  # set to local .pth under /kaggle/input/... if you uploaded weights\n",
    "    pretrained_url='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',   # optional; if empty, uses a known default for realesrgan_x2plus\n",
    "    allow_download=True,  # set True only if Kaggle internet is enabled\n",
    "    model='rrdb_lite',     # fallback internal model\n",
    "    rrdb_blocks=6,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    patch_size_hr=128,\n",
    "    lambda_cycle=0.25,  # degradation-consistency (paired lr/hr only)\n",
    "    cycle_blur_sigma=1.2,\n",
    " )\n",
    "sr_out_dir = OUT_ROOT / 'sr'\n",
    "sr_ckpt = sr_out_dir / 'sr_best.pt'\n",
    "\n",
    "# Build SR checkpoint (pretrained if possible; otherwise train fallback)\n",
    "if not sr_ckpt.exists():\n",
    "    try:\n",
    "        sr_ckpt = train_sr(DATA_ROOT, sr_cfg, out_dir=sr_out_dir, max_steps_per_epoch=500)\n",
    "    except Exception as e:\n",
    "        print('Pretrained SR unavailable (falling back to internal training):', repr(e))\n",
    "        sr_cfg.use_pretrained = False\n",
    "        sr_ckpt = train_sr(DATA_ROOT, sr_cfg, out_dir=sr_out_dir, max_steps_per_epoch=500)\n",
    "\n",
    "print('SR checkpoint:', sr_ckpt)\n",
    "\n",
    "# Export SR images\n",
    "# For LR distribution consistency, use SR_SOURCE='lr' (recommended).\n",
    "# Use 'hr' only if you explicitly want HR -> HR+.\n",
    "SR_SOURCE = 'lr'  # 'lr' | 'hr' | 'auto'\n",
    "\n",
    "# Set LIMIT_TRACKS to e.g. 200 for quick experiments\n",
    "LIMIT_TRACKS = None\n",
    "hr_plus_root = export_hr_plus(\n",
    "    DATA_ROOT,\n",
    "    sr_ckpt,\n",
    "    hr_plus_root=OUT_ROOT / 'hr_plus',\n",
    "    scale=sr_cfg.scale,\n",
    "    source=SR_SOURCE,\n",
    "    limit_tracks=LIMIT_TRACKS,\n",
    " )\n",
    "print('HR+ root:', hr_plus_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9145e4",
   "metadata": {},
   "source": [
    "## 3) OCR Training (Multi-frame CRNN + CTC)\n",
    "We train a CRNN on 5-frame tracks. If `hr_plus_root` exists, the dataloader will automatically use **HR frames (if available)** and swap them to **HR+** when exported (falls back to LR otherwise).\n",
    "\n",
    "### CTC design notes for Brazil/Mercosur\n",
    "- Labels are normalized to uppercase alnum and canonicalized by stripping `-`\n",
    "- CTCLoss uses `blank=0` and `zero_infinity=True`\n",
    "- Decoding uses **beam search** + **plate template constraints** and a confusion-map to handle ambiguous characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_cfg = OCRConfig(\n",
    "    data_root=DATA_ROOT,\n",
    "    frames_per_sample=5,\n",
    "    img_height=32,\n",
    "    img_width=128,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    num_workers=2,\n",
    "    learning_rate=1e-3,\n",
    " )\n",
    "\n",
    "# Prefer 'mercosur' if most of your dataset is Mercosur format\n",
    "PREFER_TEMPLATE = None  # or 'mercosur' or 'brazil_old'\n",
    "\n",
    "ocr_out = train_ocr(\n",
    "    ocr_cfg,\n",
    "    out_dir=OUT_ROOT / 'ocr',\n",
    "    hr_plus_root=str(hr_plus_root.parent),  # points to .../hr_plus\n",
    "    hr_plus_scale=sr_cfg.scale,\n",
    "    split_ratio=0.9,\n",
    "    prefer_template=PREFER_TEMPLATE,\n",
    " )\n",
    "ocr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9543796",
   "metadata": {},
   "source": [
    "## 4) Artifacts & quick inspection\n",
    "We save: SR checkpoint, OCR checkpoint, metrics CSV, and a small `val_samples.json` for error inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "print('Output root:', OUT_ROOT)\n",
    "print('Files:')\n",
    "for p in sorted(OUT_ROOT.rglob('*'))[:50]:\n",
    "    print(' -', p)\n",
    "\n",
    "samples_path = OUT_ROOT / 'ocr' / 'val_samples.json'\n",
    "if samples_path.exists():\n",
    "    samples = json.loads(samples_path.read_text())\n",
    "    pprint(samples[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152a89c",
   "metadata": {},
   "source": [
    "## 5) (Optional) Zip outputs for download\n",
    "Kaggle lets you download the zip from the Output panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "zip_path = str(OUT_ROOT) + '.zip'\n",
    "if Path(zip_path).exists():\n",
    "    Path(zip_path).unlink()\n",
    "shutil.make_archive(str(OUT_ROOT), 'zip', root_dir=str(OUT_ROOT))\n",
    "print('Zipped to:', zip_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
